---
title: "The main drivers of readmission of diabetes patients"
author: "Harold Armando Achicanoy Estrella"
date: "12/01/2020"
output: html_document
---

## Overview

The main objective of this analysis consists of identifying the variables that contribute the most to explain why some patients reconsulted after the application of diabetes treatments. The [dataset](https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008#) is composed of hospital records from 130 units from the hospital network in the USA through the period 1999-2008.

Understanding the readmission process causes will help doctors to support decision making on how to address and treat diabetes patients.

To address this objective the idea is to follow a data science process applying some data preparation, description, feature selection, and finally the execution of some classification models. Put some highlighted results.

The carried analysis uses the **[Spark 2.1.0](https://spark.apache.org/docs/2.1.0/)** general-purpose cluster computing system to run the classification models. The machine configuration is as follows: MacBook Pro (13-inch, Mid 2012), Processor: 2.5 GHz Intel Core i5, RAM: 16 GB.

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
# R options
options(warn = -1, scipen = 999)
if(!require(pacman)){install.packages('pacman')}
suppressMessages(library(pacman))
suppressMessages(pacman::p_load(tidyverse, vroom, psych,
                                caret, caretEnsemble, corrplot,
                                ranger, fastcluster, sparklyr,
                                fastDummies, ape, Boruta,
                                future, future.apply, furrr,
                                lsr, RColorBrewer, DT, skimr))
```

The structure of this notebook is divided in the following sections: data obtention, data pre-processing, descriptive analysis, correlation analysis, feature selection, and models fitting to identify the main drivers.

## 1. Data obtention

The first step consists in obtain the data from the web source, unzip the compressed file, and read it using a fast library to load text files in R.

```{r obtention, include = TRUE}
## --------------------------------------------------- ##
## Data obtention
## --------------------------------------------------- ##
if(!file.exists(paste0(getwd(),'/dataset_diabetes/diabetic_data.csv'))){
  ## Download data
  url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/00296/dataset_diabetes.zip'
  download.file(url, destfile = paste0(getwd(),'/dataset_diabetes.zip'))
  ## Unzip files
  unzip(paste0(getwd(),'/dataset_diabetes.zip'))
  file.remove(paste0(getwd(),'/dataset_diabetes.zip'))
  ## Load the data
  tbl <- vroom::vroom(paste0(getwd(),'/dataset_diabetes/diabetic_data.csv'), delim = ',')
} else {
  ## Load the data
  tbl <- vroom::vroom(paste0(getwd(),'/dataset_diabetes/diabetic_data.csv'), delim = ',')
}
## Replace '?' character by NA's
tbl <- tbl %>% dplyr::na_if(y = '?')
```

The dataset is conformed by 101766 patients-records with 50 attributes of information.

```{r dimensions, include = TRUE}
cat(paste0('Dataset dimensions\n'))
print(dim(tbl))
```

## 2. Data pre-processing

The data pre-processing steps consist in:

* Identify and remove features without or low variance
* Replace codes by the right categories in some categorical variables
* Regroup many labels into meaningful categories
* Identify and remove features with more than 25% of missing data
* Check the data types
* Create or modify variables

```{r preprocessing, include = TRUE}
## --------------------------------------------------- ##
## Data pre-processing
## --------------------------------------------------- ##
## Identify and remove variables without or with low variance
zvar <- tbl %>% caret::nzv()
cat(paste0('Removed features due to the low or null variance\n'))
print(names(tbl)[zvar])
tbl  <- tbl[,-zvar]; rm(zvar)

## Replace the codes by the right categories to the *admission* variables
# Load ID identifiers
idi  <- vroom::vroom(paste0(getwd(),'/dataset_diabetes/IDs_mapping.csv'), delim = ',')
mtch <- which(is.na(idi$admission_type_id))
# admision_type_id
idi1 <- idi[1:(mtch[1]-1),]
# discharge_disposition_id
idi2 <- idi[(mtch[1]+1):(mtch[2]-1),]
names(idi2) <- as.character(idi2[1,])
idi2 <- idi2[-1,]
# admission_source_id
idi3 <- idi[(mtch[2]+1):nrow(idi),]
names(idi3) <- as.character(idi3[1,])
idi3 <- idi3[-1,]; rm(mtch)

tbl$admission_type_id <- factor(tbl$admission_type_id)
levels(tbl$admission_type_id) <- idi1$description
levels(tbl$admission_type_id)[levels(tbl$admission_type_id) %in% c('NULL','Not Available')] <- NA
# Create a new category which comprises the levels: 'Newborn','Trauma Center','Not Mapped'
levels(tbl$admission_type_id)[levels(tbl$admission_type_id) %in% c('Newborn','Trauma Center','Not Mapped')] <- 'Other'

tbl$discharge_disposition_id <- factor(tbl$discharge_disposition_id)
levels(tbl$discharge_disposition_id) <- idi2$description[match(levels(tbl$discharge_disposition_id), idi2$discharge_disposition_id)]
levels(tbl$discharge_disposition_id)[levels(tbl$discharge_disposition_id) %in% c('NULL','Not Available')] <- NA
# Create new categories: Discharged, Expired, Hospice, and Admitted
levels(tbl$discharge_disposition_id)[grep(pattern = '[dD][iI][sS][cC][hH][aA][rR][gG][eE][dD]', x = levels(tbl$discharge_disposition_id))] <- 'Discharged'
levels(tbl$discharge_disposition_id)[grep(pattern = '[eE][xX][pP][iI][rR][eE][dD]', x = levels(tbl$discharge_disposition_id))] <- 'Expired'
levels(tbl$discharge_disposition_id)[grep(pattern = '[hH][oO][sS][pP][iI][cC][eE]', x = levels(tbl$discharge_disposition_id))] <- 'Hospice'
levels(tbl$discharge_disposition_id)[c(2,3,5)] <- 'Admitted'

tbl$admission_source_id <- factor(tbl$admission_source_id)
levels(tbl$admission_source_id) <- idi3$description[match(levels(tbl$admission_source_id), idi3$admission_source_id)]
levels(tbl$admission_source_id)[levels(tbl$admission_source_id) %in% c('NULL','Not Available')] <- NA
# Create new categories: Transfer, Referral, and Other
levels(tbl$admission_source_id)[grep(pattern = '[tT][rR][aA][nN][sS][fF][eE][rR]', x = levels(tbl$admission_source_id))] <- 'Transfer'
levels(tbl$admission_source_id)[grep(pattern = '[rR][eE][fF][eE][rR][rR][aA][lL]', x = levels(tbl$admission_source_id))] <- 'Referral'
levels(tbl$admission_source_id)[4:8] <- 'Other'
rm(idi, idi1, idi2, idi3)

## Omit Unknown category in gender
# It only has 3 registers
cat(paste0('Omit Unknown category in gender due to low variation\n'))
print(table(tbl$gender))
tbl$gender[grep(pattern = 'Unknown', x = tbl$gender)] <- NA

## Identify variables with more than 25% of missing data
msg <- tbl %>%
  apply(X = ., MARGIN = 2, function(x){sum(is.na(x))/nrow(.)}) %>%
  sort(decreasing = T) %>%
  .[. > 0.25]
cat(paste0('Removed features due to high proportion of missing data\n'))
print(msg)

## Remove variables with more than 25% of missing data
tbl <- tbl[,-which(names(tbl) %in% names(msg))]; rm(msg)

## Create a new feature: How many times the same patient have visited the hospital?
# Select the ones with the maximum number of days interned in the hospital
visits <- tbl$patient_nbr %>% table %>% sort(decreasing = T) %>% base::as.data.frame()
names(visits)[1] <- 'patient_nbr'
visits$patient_nbr <- visits$patient_nbr %>% as.character() %>% as.numeric()
unq_vs <- visits %>% dplyr::filter(Freq == 1)
visits <- visits %>% dplyr::filter(Freq > 1)

tbl_unq <- tbl %>% dplyr::filter(patient_nbr %in% unq_vs$patient_nbr)
tbl_unq$number_visits <- 1

tbl_dup <- 1:nrow(visits) %>%
  purrr::map(.f = function(i){
    df <- tbl %>%
      dplyr::filter(patient_nbr == visits$patient_nbr[i]) %>%
      .[which.max(.$time_in_hospital)[1],]
    df$number_visits <- visits$Freq[i]
    return(df)
  }) %>%
  dplyr::bind_rows()

tbl <- rbind(tbl_dup, tbl_unq)
# Number of unique patients
cat(paste0('Unique number of patients after removing duplicated information\n'))
print(nrow(tbl))
rm(tbl_dup, tbl_unq, unq_vs, visits)

## Transform character to factors
tbl[sapply(tbl, is.character)] <- lapply(tbl[sapply(tbl, is.character)], as.factor)

## Create a new feature: numerical age using the midpoint of each age interval
tbl$age_num <- tbl$age %>%
  gsub('\\[', '', .) %>%
  gsub('\\)', '', .) %>%
  strsplit(., split = '-') %>%
  purrr::map(.f = function(int){
    x <- as.numeric(int)
    return(mean(x))
  }) %>%
  unlist

# The following features: diag_1, diag_2, and diag_3 make
# reference of the three initial detected diagnoses. They
# have more than 700 categories. Additionally, the feature
# 'number_diagnoses' captures the sum of all of detected
# diagnoses. For that reason, diag_1,diag_2, and diag_3
# were excluded of the analysis
tbl$diag_1 <- tbl$diag_2 <- tbl$diag_3 <- NULL

# Discard expired and hospiced people from analysis
tbl$discharge_disposition_id <- tbl$discharge_disposition_id %>% as.character
tbl <- tbl %>%
  dplyr::filter(!(discharge_disposition_id %in% c('Expired','Hospice')))
tbl$discharge_disposition_id <- tbl$discharge_disposition_id %>% factor()

## Response variable
# Considering that the readmission before and after 30 days
# are both bad situations for a patient. The analysis will
# be carried out using a binary classification, merging the
# admissions before and after 30 days in one category
tbl$readmitted <- ifelse(test = tbl$readmitted %in% c('<30','>30'),
                         yes  = 1,
                         no   = 0)
# Data proportion of response category
cat(paste0('Response variable proportion\n'))
print(table(tbl$readmitted)/nrow(tbl))

# Remove ID variables
tbl$encounter_id <- NULL
tbl$patient_nbr  <- NULL

# Transform categorical variables to dummies 
tbl_num <- tbl %>% fastDummies::dummy_cols(ignore_na = T)
tbl_num <- tbl_num %>%
  dplyr::select(time_in_hospital:number_diagnoses,
                number_visits,age_num,
                race_AfricanAmerican:race_Other,
                gender_Female,gender_Male,
                admission_type_id_Emergency:admission_type_id_Other,
                discharge_disposition_id_Admitted:`discharge_disposition_id_Not Mapped`,
                admission_source_id_Referral:admission_source_id_Other,
                `A1Cresult_>7`:diabetesMed_Yes,
                readmitted)

# Based on the proportion of complete observations which is 82%
# the final decision is not impute missing data
cat('Proportion of complete observations\n')
print(nrow(tbl_num[complete.cases(tbl_num),])/nrow(tbl_num))

tbl_num_full <- tbl_num %>% tidyr::drop_na()

# Data proportion of response category
cat(paste0('Response variable proportion\n'))
print(table(tbl_num_full$readmitted)/nrow(tbl_num_full))

## Identify and remove categories without or with low variance
zvar <- tbl_num_full %>% caret::nzv()
cat('Dummy categories without or with low variance\n')
print(names(tbl_num_full)[zvar])
tbl_num_full <- tbl_num_full[,-zvar]; rm(zvar)

cat(paste0('Dataset dimensions\n'))
print(dim(tbl_num_full))

out_dir <- paste0(getwd(),'/processed_data')
if(!dir.exists(out_dir)){dir.create(out_dir, recursive = T)}

if(!file.exists(paste0(out_dir,'/diabetic_data_processed_1.csv'))){
  vroom::vroom_write(x = tbl, paste0(out_dir,'/diabetic_data_processed_1.csv'), delim = ',')
}
if(!file.exists(paste0(out_dir,'/diabetic_data_processed_2.csv'))){
  vroom::vroom_write(x = tbl_num, paste0(out_dir,'/diabetic_data_processed_2.csv'), delim = ',')
}
if(!file.exists(paste0(out_dir,'/diabetic_data_processed_2_complete.csv'))){
  vroom::vroom_write(x = tbl_num_full, paste0(out_dir,'/diabetic_data_processed_2_complete.csv'), delim = ',')
}
rm(out_dir)
```

The important highlights is this section are:

* 18 features removed due to near zero variability
* 3 features removed due to high percentage of missing data
* Multiple records per patient are present in the dataset. That represents multiple visits to the hospital network. These records were summarized in just one per patient. Selecting the record with the maximum number of days interned in the hospital and creating a new variable called: *number_visits*
* 71518 unique patient records obtained after removing duplicate records per patient
* Age numerical variable obtained using the midpoint of the age categorical intervals
* The first three diagnoses which are categorical variables with more than 700 categories were discarded. The variable *number_diagnoses* captures the sum of all of detected diagnoses
* The people classified as Expired or Hospiced were omitted from the analysis due to they can not be readmitted
* The response variable was transformed to binary output. Considering that the readmission before and after 30 days are both bad situations for a patient. The analysis will be carried out using a binary classification, merging the readmissions before and after 30 days in one category
* The proportion of the response variables is: readmitted (35%) vs no readmitted (65%)
* All the categorical variables were transformed to dummies
* The percentage of complete data is: 82.7%. Considering that this is a high proportion of complete cases, missing data were not imputed
* The final dataset contains 57642 patient records with 42 features

## 3. Exploratory Data Analysis

This step consists in checking the type of data and develop the final

```{r description}
## --------------------------------------------------- ##
## Exploratory Data Analysis
## --------------------------------------------------- ##

# This function brings a complete description of the dataset
# for both categorical and numerical variables
skimr::skim(tbl)
```

## 4. Correlation Analysis

```{r corr, include = TRUE}
## --------------------------------------------------- ##
## Correlation Analysis
## --------------------------------------------------- ##

# This function approximates the correlation calculation for
# mixed variables (numerical and categorical). The following
# cases are considered:
# - Both variables numeric: non-parametric Spearman correlation
# - One variable categorical, the other one numerical: ANOVA
# test getting the R2 and applying the square root
# - Both variables categorical: Cramer's V test to measure
# association between two nominal variables
# Taken from https://gist.github.com/talegari/b514dbbc651c25e2075d88f31d48057b
source(paste0(getwd(),'/scripts/cor2_modified.R'))

m <- cor2(df = tbl_num_full)
corrplot::corrplot(m,
                   type   = "upper",
                   method = "square",
                   order  = "hclust",
                   col    = brewer.pal(n = 8, name = "RdBu"),
                   tl.cex	= .5)
```

## 5. Feature Selection

```{r fselection, include = TRUE}
## --------------------------------------------------- ##
## Feature selection
## --------------------------------------------------- ##

out_dir <- paste0(getwd(),'/results')
if(!dir.exists(out_dir)){dir.create(out_dir, recursive = T)}
if(!file.exists(paste0(out_dir,'/feature_selection_simulation.csv'))){
  
  # Generate a local cluster of processors
  future::plan(multiprocess, workers = future::availableCores()-1)
  # Define a seed to do the results replicable
  set.seed(1)
  # Generate 20 random seeds from a uniform distribution
  seeds <- round(runif(20) * 1000, 0)
  # Run Boruta algorithm of feature selection using 20
  # random subsets of 2000 patients information and
  # save final decision: features CONFIRMED, TENTATIVE,
  # and REJECTED
  selected_fts <- seeds %>%
    furrr::future_map(.f = function(seed){
      # Fix each seed
      set.seed(seed)
      # Obtain a random sample without replacement of 2000 observations
      smp <- sample(x = 1:nrow(tbl_num_full), size = 2000, replace = F)
      # Run Boruta algorithm of feature selection over this random sample
      fts <- Boruta::Boruta(formula = readmitted ~ ., data = tbl_num_full[smp,])
      # Get the final decision
      res <- fts$finalDecision
      return(res)
    })
  
  fts <- selected_fts %>% as.data.frame()
  fts$Feature <- rownames(fts)
  rownames(fts) <- 1:nrow(fts)
  colnames(fts)[1:20] <- paste0('Run_',1:20)
  
  write.csv(fts, paste0(out_dir,'/feature_selection_simulation.csv'), row.names = F)
} else {
  fts <- read.csv(paste0(out_dir,'/feature_selection_simulation.csv'))
}

fts %>%
  tidyr::pivot_longer(cols = Run_1:Run_20) %>%
  dplyr::group_by(Feature, value) %>%
  dplyr::summarise(Count = n()) %>%
  ggplot2::ggplot(aes(x = Feature, y = Count, fill = value)) +
  ggplot2::scale_fill_brewer(palette = 'Set1', direction = -1) +
  ggplot2::geom_bar(stat = 'identity') +
  ggplot2::coord_flip() +
  ggplot2::labs(fill = 'Decision')

fnl_fst <- fts %>%
  tidyr::pivot_longer(cols = Run_1:Run_20) %>%
  dplyr::group_by(Feature, value) %>%
  dplyr::summarise(Count = n()) %>%
  dplyr::ungroup() %>%
  dplyr::group_by(Feature) %>%
  dplyr::summarise(Decision = value[which.max(Count)]) %>%
  dplyr::filter(Decision %in% c('Confirmed','Tentative')) %>%
  dplyr::pull(Feature)
fnl_fst[1] <- gsub('\`','',fnl_fst[1])
cat('Selected features from Boruta algorithm\n')
print(fnl_fst)
```

## 6. Classification Analysis

## Conclusions
